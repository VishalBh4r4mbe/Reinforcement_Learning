{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ_ACTION_SPACE = env.action_space.n\n",
    "SZ_OBS_SPACE = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    def __init__(self,env,alpha=0.1 ,  gamma=0.99,epsilon=1,epsilon_decay_dec = 0.001,min_epsilon = 0.01):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay_dec\n",
    "        self.Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "        self.min_epsilon = min_epsilon\n",
    "    def run(self,episodes):\n",
    "        state = self.env.reset()\n",
    "        success_rate = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                if np.random.uniform(0,1)<self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.Q[state,:])\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                self.Q[state,action]= (1-self.alpha)*self.Q[state,action] + self.alpha*(reward + self.gamma*np.max(self.Q[next_state,:]))\n",
    "                state = next_state\n",
    "            self.epsilon = max(self.min_epsilon, np.exp(-self.epsilon_decay*episode))\n",
    "            \n",
    "            \n",
    "    def evaluate(self,episodes):\n",
    "        total_reward = 0\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.argmax(self.Q[state,:])\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "        return total_reward/episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"FrozenLake-v0\"\n",
    "env = gym.make(env_name)\n",
    "x = QLearner(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47353034, 0.45957649, 0.46133681, 0.45795379],\n",
       "       [0.31451843, 0.36923785, 0.33471545, 0.42261253],\n",
       "       [0.40177954, 0.39759038, 0.37314446, 0.40593967],\n",
       "       [0.30250821, 0.23689135, 0.24378612, 0.39862575],\n",
       "       [0.49552109, 0.34896256, 0.25260687, 0.27780119],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.34614926, 0.19814225, 0.18092307, 0.16099492],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.35592092, 0.43026885, 0.38395919, 0.53596223],\n",
       "       [0.4986949 , 0.59041604, 0.51573299, 0.47652002],\n",
       "       [0.64148871, 0.38534917, 0.34256716, 0.39987162],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.40979281, 0.57443168, 0.66077835, 0.56945183],\n",
       "       [0.7274544 , 0.82849   , 0.73206156, 0.72575282],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.evaluate(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
